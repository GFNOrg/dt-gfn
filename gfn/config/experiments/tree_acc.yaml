# @package _global_
defaults:
   - override /env: tree
   - override /gflownet: trajectorybalance
   - override /policy: mlp
   - override /proxy: tree_dirichlet
   - override /logger: wandb
   - override /user: user

# Miscellaneous
device: gpu
float_precision: 32
n_samples: 1
SAVE: False

# Environment
env:
  _target_: gflownet.envs.tree_acc.Tree 
  dataset: iris 
  split_seed: 1
  data_path: ${user.data.root}/${env.dataset}/${env.dataset}_${env.split_seed}.csv # Add desired data path
  max_depth: 3
  continuous: False
  beta_binomial: False
  dirichlet: True
  mask_redundant_choices: True
  reward_func: dummy
  prior: [.1, .1, .1]
  n_classes: 3
  n_thresholds: 9
  policy_format: mlp
  threshold_components: 3
  test_args:
    top_k_trees: 1
  buffer:
    min_replay_reward: -100000
    replay_capacity: 1

# GFlowNet hyperparameters
gflownet:
  _target_: gflownet.gflownet_acc.GFlowNetAgent
  logreward: False
  use_mixed_precision: True
  random_action_prob: 0.1
  replay_sampling: weighted
  optimizer:
    batch_size:
      forward: 90
      backward_dataset: 0
      backward_replay: 10
    lr: 0.001
    z_dim: 16
    lr_z_mult: 100
    n_train_steps: 200
    lr_decay_period: 1000

# MLP policy
policy:
  shared:
    type: mlp
    n_hid: 256
    n_layers: 3
  forward: null
  backward:
    shared_weights: False

# Proxy
proxy:
  _target_: gflownet.proxy.tree_acc.CategoricalTreeProxy
  use_prior: False
  log_likelihood_only: False
  beta: 0
  alpha: Uniform
  alpha_value: .1
  mini_batch: False
  batch_size: 512

# WandB
logger:
  lightweight: True
  project_name: "RF-GFN"
  run_name: ${env.dataset}_${env.split_seed}_depth=${env.max_depth}_PP=${proxy.alpha}/${proxy.alpha_value}_SP=${proxy.use_prior}_${n_samples} # Default wandb name
  tags:
    - gflownet
  test:
    period: 100 
    n: 500
  checkpoints:
    period: 1000

# Hydra
hydra:
  run:
    dir: ${user.logdir.root}/debug/${env.dataset}_${env.split_seed}_max_depth=${env.max_depth}/${n_samples} # Default directory